{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import itertools\n",
    "from pymatreader import read_mat\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score , roc_auc_score, recall_score,precision_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "#import classifiers from scikit-learn\n",
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,StackingClassifier,VotingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa # explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import time # tic = time.perf_counter() ,toc = time.perf_counter()\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "    \n",
    "#number of splits for cross validation\n",
    "number_of_n_splits=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o fakelos poy periexei ta dataset kai einai ths morfhs class-data\n",
    "dataDir =r\"C:/Users/John/Desktop/machine learning/dataset/διπλωματικη/\"\n",
    "#dhmiourgia listas poy tha valoyme ta datasets\n",
    "matlab_file = []\n",
    "#vazomye ola ta arxeia poy periexontai sth dieuthnish 'dataDir' sth lista poy dhmioyrghsame\n",
    "#epeidh einai arxeia ths matlab, xrhsimopoioyme th entolh read_mat gia th metatroph_\n",
    "for file in os.listdir(dataDir) :\n",
    "    matlab_file.append(read_mat(dataDir+file))\n",
    "\n",
    "#save the names of datasets to only_names\n",
    "only_names = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/dataset/διπλωματικη/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/dataset/διπλωματικη/\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apotelesmata_df.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/f1_score.xlsx')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#epeidh otan gyrizw ta apotelesmata apo to excel einai ola apothikeumena se string eftia3a funtion na ta gyrizei se float\n",
    "apotelesmata_df_accuracy_score=pd.read_excel('C:/Users/John/Desktop/machine learning/apotelesmata/accuracy_score.xls')\n",
    "\n",
    "apotelesmata_df_f1_score=pd.read_excel('C:/Users/John/Desktop/machine learning/apotelesmata/f1_score.xls') \n",
    "\n",
    "apotelesmata_df_specificity=pd.read_excel('C:/Users/John/Desktop/machine learning/apotelesmata/specificity.xls') \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_function():\n",
    "  y=AdaBoostClassifier()\n",
    "  return y;\n",
    "\n",
    "def Voting_function():\n",
    "  y=VotingClassifier(estimators=[('BaggingClassifier', BaggingClassifier()), ('RandomForest', RandomForestClassifier()), ('ExtraTreesClassifier',ExtraTreesClassifier())], voting='hard')\n",
    "  return y;\n",
    "\n",
    "def Bagging_function():\n",
    "  y=BaggingClassifier()\n",
    "  return y;\n",
    "\n",
    "def ExtraTrees_function():\n",
    "  y=ExtraTreesClassifier()\n",
    "  return y;\n",
    "\n",
    "def RandomForest_function():\n",
    "  y=RandomForestClassifier()\n",
    "  return y;\n",
    "\n",
    "def Stacking_function():\n",
    "   estimators = [( RandomForestClassifier()),( ExtraTreesClassifier())]\n",
    "   y = StackingClassifier(estimators=estimators, final_estimator=GaussianNB())\n",
    "   return y;\n",
    "\n",
    "def GradientBoosting_function():\n",
    "  y=GradientBoostingClassifier()\n",
    "  return y;\n",
    "\n",
    "def LightGBM_function (data_train, class_train,data_test, parameters):\n",
    "    d_train = lgb.Dataset(data_train, label= class_train)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(parameters, d_train, 500, \n",
    "                      valid_sets=[d_train],\n",
    "                      early_stopping_rounds=100, \n",
    "                       verbose_eval=50, \n",
    "                       evals_result=evals_result)\n",
    "     \n",
    "    predicted = model.predict(data_test, num_iteration=500)\n",
    "    \n",
    "    predicted_labels=np.argmax(predicted,axis=1);\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "def LightGBM_function_proba (data_train, class_train,data_test, parameters):\n",
    "    model = LGBMClassifier(objective='multiclass',metric = 'multi_logloss',num_class=class_num+1,n_estimators=500)\n",
    "    model.fit(data_train, class_train)\n",
    "    predicted_labels=model.predict_proba(data_test)\n",
    "    \n",
    "    return predicted_labels\n",
    "      \n",
    "\n",
    "def Catboost_function (data_train, class_train,data_test,class_test, parameters):\n",
    "    model = CatBoostClassifier(**parameters)\n",
    "    train_dataset = Pool(data=data_train,label=class_train)\n",
    "    eval_dataset = Pool(data=data_test,label=class_test) \n",
    "    model.fit(train_dataset)\n",
    "    predicted_labels = model.predict(eval_dataset)\n",
    "    predicted_labels = np.reshape(predicted_labels, (1, len(predicted_labels)))\n",
    "    predicted_labels=np.squeeze(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "def Catboost_function_proba (data_train, class_train,data_test,class_test, parameters):\n",
    "    model = CatBoostClassifier(**parameters)\n",
    "    train_dataset = Pool(data=data_train,label=class_train)\n",
    "    eval_dataset = Pool(data=data_test,label=class_test) \n",
    "    model.fit(train_dataset)\n",
    "    predicted_labels = model.predict_proba(eval_dataset)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def XGBoost_function (data_train, class_train,data_test,class_test, parameters):\n",
    "   tr_data = xgb.DMatrix(data=data_train, label=class_train)\n",
    "   dtest = xgb.DMatrix(data=data_test,label=class_test)\n",
    "   watchlist = [(dtest, 'test'),(tr_data, 'train')]\n",
    "   model_xgb = xgb.train(parameters, tr_data, 500,watchlist, early_stopping_rounds = 100, verbose_eval=50)\n",
    "   predicted = model_xgb.predict(dtest,ntree_limit=model_xgb.best_ntree_limit)\n",
    "   predicted_labels=np.argmax(predicted,axis=1);\n",
    "   return predicted_labels\n",
    "\n",
    "def XGBoost_function_proba (data_train, class_train,data_test,class_test, parameters):\n",
    "    \n",
    "   clf = xgb.XGBClassifier(n_estimators = 500, objective= 'multi:softprob')\n",
    "   model_xgb=clf.fit(data_train, class_train,  early_stopping_rounds=100, eval_metric='mlogloss', eval_set=[(data_test, class_test)])\n",
    "   predicted_labels = model_xgb.predict_proba(data_test)\n",
    "   return predicted_labels\n",
    "\n",
    "def string_to_float(a):\n",
    "  a=a[:-1]\n",
    "  a=a[1:]\n",
    "  a=a.split(\",\")\n",
    "  y=[float(i) for i in a]\n",
    "  return y;\n",
    "\n",
    "def ypologismos_specificity (m):\n",
    "    if len(m)>0 :\n",
    "      TP_temp=0;\n",
    "      FP_temp=0;\n",
    "      FN_temp=0;\n",
    "      TN_temp=0;\n",
    "      for i in range(0, len(m), 4):\n",
    "          TP_temp=m[i]+TP_temp;\n",
    "          FP_temp=m[i+1]+FP_temp;\n",
    "          FN_temp=m[i+2]+FN_temp;\n",
    "          TN_temp=m[i+3]+TN_temp; \n",
    "      TP=TP_temp;\n",
    "      FP=FP_temp;\n",
    "      FN=FN_temp;\n",
    "      TN=TN_temp;\n",
    "      #creating the confusion matrix\n",
    "      #total_confusion_matrix=[[TP,FP],[FN,TN]]\n",
    "      specificity_is=TN/(TN+FP)\n",
    "    else:\n",
    "      specificity_is=0\n",
    "    \n",
    "    return specificity_is\n",
    "\n",
    "\n",
    "def voting(x,y,z) :      \n",
    "        if (x == y) : \n",
    "            value=x\n",
    "        elif (x == z) :\n",
    "            value=x\n",
    "        elif (y == z):\n",
    "            value=y;\n",
    "        else :\n",
    "           value=y\n",
    "        return value \n",
    "\n",
    "def Voting_G3(data_train, class_train,data_test,class_test, class_num, parameters_LighGBM,parameters_Catboost,parameters_XGBoost):\n",
    "\n",
    "  #oi klaseis toy dataset\n",
    "  classes=np.unique(class_df)\n",
    "  #ypologismos pithanotitwn apo to kathe algorithmo\n",
    "  predicted_LightGBM=LightGBM_function_proba (data_train, class_train,data_test, parameters_LightGBM)\n",
    "  predicted_Catboost=Catboost_function_proba (data_train, class_train,data_test,class_test, parameters_Catboost)\n",
    "  predicted_XGBoost=XGBoost_function_proba (data_train, class_train,data_test,class_test, parameters_XGBoost)\n",
    "  \n",
    "  \n",
    "  samples=len(predicted_LightGBM/len(classes))\n",
    "  predicted_labels=[0]*samples\n",
    "    \n",
    "  for j in range (0, len(predicted_LightGBM), len(classes)):\n",
    "                   proba=[]\n",
    "                   for h in range (len(classes)):\n",
    "                      proba.append( predicted_LightGBM[j+h]+predicted_Catboost[j+h]+predicted_XGBoost[j+h])\n",
    "                   #find the possition of the  biggest probability\n",
    "                   predicted_labels[j]= classes[proba1.index(max_value)]\n",
    "                    \n",
    "  predicted_labels = np.reshape(predicted_labels, (1, len(predicted_labels)))\n",
    "  predicted=np.squeeze(predicted_labels)                 \n",
    "\n",
    "  return predicted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dhmiourgia ndarray me theseis osoi kai oi classifiers poy tha xrhsimopoihsw apo th vivliothiki sklearn\n",
    "clf=np.empty((7), dtype=object)\n",
    "\n",
    "# Create adaboost classifer \n",
    "clf[0] = AdaBoost_function()\n",
    "\n",
    "#Create Bagging Classifier \n",
    "clf[1] = Bagging_function()\n",
    "\n",
    "#Create ExtraTreesClassifier\n",
    "clf[2] = ExtraTrees_function()\n",
    "\n",
    "#Create RandomForestClassifier\n",
    "clf[3] = RandomForestClassifier()#RandomForest_function()\n",
    "\n",
    "#Create StackingClassifier , Stack of estimators with a final classifier\n",
    "#clf[4] = Stacking_function()\n",
    "estimators = [( RandomForestClassifier()),( ExtraTreesClassifier())]\n",
    "clf[4]= StackingClassifier(estimators=estimators, final_estimator=GaussianNB())\n",
    "\n",
    "#Create VotingClassifier\n",
    "clf[5] = Voting_function()\n",
    "\n",
    "#Create GradientBoostingClassifier\n",
    "clf[6] = GradientBoosting_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range (len(matlab_file)):\n",
    "  data_df = pd.DataFrame(matlab_file[t]['data'])\n",
    "  class_df = pd.DataFrame(matlab_file[t]['class'])\n",
    "  \n",
    "  \n",
    "  # Creating an empty dictionary to store results\n",
    "  apotelesmata = {}\n",
    "  apotelesmata[\"key0\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] \n",
    "  apotelesmata[\"key1\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] \n",
    "  apotelesmata[\"key2\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "  apotelesmata[\"key3\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] \n",
    "  apotelesmata[\"key4\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] \n",
    "  apotelesmata[\"key5\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] \n",
    "  apotelesmata[\"key6\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "  h=-1;\n",
    "  print(\"to t einai\",t, 'egine reset sto h',h) \n",
    "  for x,y in apotelesmata.items():\n",
    "    apotelesmata[x];\n",
    "    p=0;\n",
    "    h=h+1;\n",
    "    #choose classifier\n",
    "    classifier=clf[h]\n",
    "    print('Arxh, classifier',h , 'to x einai',x)\n",
    "    for j in range (5):\n",
    "       tic = time.perf_counter()\n",
    "       kf = KFold(n_splits=number_of_n_splits, shuffle=True)\n",
    "       #k-fold\n",
    "       for train_index, test_index in kf.split(data_df,class_df):\n",
    "          X_train, X_test = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "          y_train, y_test = class_df.iloc[train_index], class_df.iloc[test_index]\n",
    "          \n",
    "          \n",
    "          # Train Classifer\n",
    "          \n",
    "          model = classifier.fit(X_train, y_train)\n",
    "          predicted = model.predict(X_test)\n",
    "          \n",
    "          #apothikeush sth lista apotelesmatwn\n",
    "          apotelesmata[x][p+0].extend([predicted])\n",
    "          \n",
    "          #metatroph apo df se array\n",
    "          y_test=y_test.to_numpy()\n",
    "          #allagh diastasewn se 1-D\n",
    "          y_test=np.ravel(y_test)\n",
    "          \n",
    "          #apothikeush sth lista apotelesmatwn\n",
    "          apotelesmata[x][p+1].extend([y_test])\n",
    "          \n",
    "       toc = time.perf_counter()\n",
    "       time_of=toc-tic\n",
    "       #apothikeush sth lista apotelesmatwn\n",
    "       apotelesmata[x][p+2].extend([time_of])\n",
    "       p=p+3;\n",
    "       print('~~teleiwse to K-fold noymero~~',j )\n",
    "       print('xronos=',time_of)\n",
    "    print(\"telos clf\",h,\"kai to t einai\",t)\n",
    "    with open(r'C:\\Users\\John\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\apotelesmata_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "             pickle.dump(apotelesmata, fp, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "  #save the data\n",
    "  with open(r'C:\\Users\\John\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\apotelesmata_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "    pickle.dump(apotelesmata, fp, protocol=pickle.HIGHEST_PROTOCOL)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ~~ XGBoost  ~~\n",
    "\n",
    "kf = KFold(n_splits=number_of_n_splits, shuffle=True)\n",
    "#print('im in')\n",
    "\n",
    "\n",
    "for t in range (len(matlab_file)):\n",
    "  data_df = pd.DataFrame(matlab_file[t]['data'])\n",
    "  class_df = pd.DataFrame(matlab_file[t]['class'])\n",
    "  class_num=class_df[0].nunique()\n",
    "  parameters = {'objective': 'multi:softprob', \n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class':class_num +1,\n",
    "          'learning_rate': 0.1\n",
    "          }\n",
    "  \n",
    "  # Creating an empty dictionary to store results\n",
    "  apotelesmata_Xgboost = {}\n",
    "  apotelesmata_Xgboost [\"key0\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "  p=0;\n",
    "  print('to arxeio einai to noymero',t )\n",
    "  for j in range (5):\n",
    "           tic = time.perf_counter()\n",
    "    \n",
    "#k-fold         \n",
    "           for train_index, test_index in kf.split(data_df,class_df):\n",
    "                     X_train, X_test = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "                     y_train, y_test = class_df.iloc[train_index], class_df.iloc[test_index]\n",
    "                     \n",
    "                     \n",
    "                     predicted=XGBoost_function (X_train, y_train,X_test,y_test, parameters)\n",
    "                     \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_Xgboost [\"key0\"][p+0].extend([predicted])\n",
    "          \n",
    "                     #metatroph apo df se array\n",
    "                     y_test=y_test.to_numpy()\n",
    "                \n",
    "                     #allagh diastasewn se 1-D\n",
    "                     y_test=np.ravel(y_test)\n",
    "          \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_Xgboost [\"key0\"][p+1].extend([y_test])\n",
    "          \n",
    "            \n",
    "            \n",
    "           toc = time.perf_counter()\n",
    "           time_of=toc-tic\n",
    "           #apothikeush sth lista apotelesmatwn\n",
    "           apotelesmata_Xgboost [\"key0\"][p+2].extend([time_of])\n",
    "           p=p+3;\n",
    "           print('~~teleiwse to K-fold noymero~~',j )\n",
    "           print('xronos=',time_of)\n",
    "           #save the data\n",
    "           with open(r'C:\\Users\\john\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\apotelesmata_XGBoost_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "                    pickle.dump(apotelesmata_Xgboost, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ~~    LightGBM   ~~\n",
    "\n",
    "kf = KFold(n_splits=number_of_n_splits, shuffle=True)\n",
    "\n",
    "#tre3imo gia kathe arxeio-dataset\n",
    "for t in range (len(matlab_file)):\n",
    "  data_df = pd.DataFrame(matlab_file[t]['data'])\n",
    "  class_df = pd.DataFrame(matlab_file[t]['class'])\n",
    "  #euresh twn monadikwn klasewn\n",
    "  class_num=class_df[0].nunique()\n",
    "  parameters = {\n",
    "        'objective' : 'multiclass',\n",
    "        'metric' : 'multi_logloss',\n",
    "        'num_class':class_num+1,\n",
    "        'learning_rate': 0.1\n",
    "    }\n",
    "  \n",
    "  # Creating an empty dictionary to store results\n",
    "  apotelesmata_LightGBM = {}\n",
    "  apotelesmata_LightGBM [\"key0\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "  p=0;\n",
    "  print('to arxeio einai to noymero',t )\n",
    "  for j in range (5):\n",
    "           tic = time.perf_counter()\n",
    "                \n",
    "#k-fold         \n",
    "           for train_index, test_index in kf.split(data_df,class_df):\n",
    "                     \n",
    "                     X_train, X_test = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "                     y_train, y_test = class_df.iloc[train_index], class_df.iloc[test_index]\n",
    "                     \n",
    "                     y_test=y_test.to_numpy()\n",
    "                     \n",
    "                     predicted=LightGBM_function(X_train,y_train,y_test,parameters)\n",
    "                                                \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_LightGBM [\"key0\"][p+0].extend([predicted])\n",
    "          \n",
    "                     #allagh diastasewn se 1-D\n",
    "                     y_test=np.ravel(y_test)\n",
    "          \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_LightGBM [\"key0\"][p+1].extend([y_test])\n",
    "          \n",
    "                     \n",
    "            \n",
    "            \n",
    "           toc = time.perf_counter()\n",
    "           #euresh xronou\n",
    "           time_of=toc-tic\n",
    "           #apothikeush sth lista apotelesmatwn\n",
    "           apotelesmata_LightGBM [\"key0\"][p+2].extend([time_of])\n",
    "           p=p+3;\n",
    "           print('~~teleiwse to K-fold noymero~~',j )\n",
    "           print('xronos=',time_of)\n",
    "           #save the data\n",
    "           with open(r'C:\\Users\\John\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\apotelesmata_LightGBM_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "                    pickle.dump(apotelesmata_LightGBM, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ~~   Catboost    ~~\n",
    "\n",
    "kf = KFold(n_splits=number_of_n_splits,random_state=50, shuffle=True)\n",
    "#print('im in')\n",
    "\n",
    "\n",
    "for t in range (len(matlab_file)):\n",
    "  data_df = pd.DataFrame(matlab_file[t]['data'])\n",
    "  class_df = pd.DataFrame(matlab_file[t]['class'])\n",
    "  parameters={'iterations':500,\n",
    "             'verbose':50,\n",
    "              'early_stopping_rounds':100,\n",
    "              'loss_function':'MultiClass',\n",
    "             'eval_metric':'MultiClass',\n",
    "              'learning_rate': 0.1\n",
    "             }\n",
    "\n",
    "  \n",
    "  # Creating an empty dictionary to store results\n",
    "  apotelesmata_Catboost = {}\n",
    "  apotelesmata_Catboost [\"key0\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "  p=0;\n",
    "  print('to arxeio einai to noymero',t )\n",
    "  for j in range (5):\n",
    "           tic = time.perf_counter()\n",
    "                \n",
    "#k-fold         \n",
    "           for train_index, test_index in kf.split(data_df,class_df):\n",
    "                     #print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "                     X_train, X_test = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "                     y_train, y_test = class_df.iloc[train_index], class_df.iloc[test_index]\n",
    "                     \n",
    "                     y_test=y_test.to_numpy()\n",
    "                     \n",
    "                     predicted=Catboost_function (X_train, y_train,X_test,y_test, parameters)  \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     predicted=np.ravel(predicted)\n",
    "                     apotelesmata_Catboost [\"key0\"][p+0].extend([predicted])\n",
    "          \n",
    "\n",
    "                     #allagh diastasewn se 1-D\n",
    "                     y_test=np.ravel(y_test)\n",
    "          \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_Catboost [\"key0\"][p+1].extend([y_test])\n",
    "          \n",
    "            \n",
    "            \n",
    "           toc = time.perf_counter()\n",
    "           time_of=toc-tic\n",
    "           #apothikeush sth lista apotelesmatwn\n",
    "           apotelesmata_Catboost [\"key0\"][p+2].extend([time_of])\n",
    "           p=p+3;\n",
    "           print('~~teleiwse to K-fold noymero~~',j )\n",
    "           print('xronos=',time_of)\n",
    "           #save the data\n",
    "           with open(r'C:\\Users\\John\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\1apotelesmata_Catboost_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "                    pickle.dump(apotelesmata_Catboost, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for G3\n",
    "\n",
    "kf = KFold(n_splits=number_of_n_splits, shuffle=True)\n",
    "#print('im in')\n",
    "  \n",
    "\n",
    "for t in range (len(matlab_file)):\n",
    "\n",
    " \n",
    " data_df = pd.DataFrame(matlab_file[t]['data'])\n",
    " class_df = pd.DataFrame(matlab_file[t]['class'])\n",
    " class_num=class_df[0].nunique()\n",
    " \n",
    " parameters_LightGBM= {\n",
    "        'objective' : 'multiclass',\n",
    "        'metric' : 'multi_logloss',\n",
    "        'num_class':class_num+1,\n",
    "        'learning_rate': 0.1\n",
    "    }\n",
    " parameters_Catboost={'iterations':500,\n",
    "             'verbose':50,\n",
    "              'early_stopping_rounds':100,\n",
    "              'loss_function':'MultiClass',\n",
    "               'learning_rate': 0.1\n",
    "             }\n",
    " parameters_XGBoost= {'objective': 'multi:softprob', \n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class':class_num +1,\n",
    "          'learning_rate': 0.1\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # Creating an empty dictionary to store results\n",
    " apotelesmata_voting_nea = {}\n",
    " apotelesmata_voting_nea [\"key0\"] = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    " p=0;\n",
    " print('to arxeio einai to noymero',t )\n",
    " for j in range (5):\n",
    "           tic = time.perf_counter()\n",
    "    \n",
    "#k-fold         \n",
    "           for train_index, test_index in kf.split(data_df,class_df):\n",
    "                     #print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "                     X_train, X_test = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "                     y_train, y_test = class_df.iloc[train_index], class_df.iloc[test_index]\n",
    "                     \n",
    "                     \n",
    "                     predicted=Voting_G3(X_train, y_train,X_test,y_test, class_num, parameters_LightGBM,parameters_Catboost,parameters_XGBoost)\n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_voting_nea [\"key0\"][p+0].extend([predicted])\n",
    "          \n",
    "                     #metatroph apo df se array\n",
    "                     y_test=y_test.to_numpy()\n",
    "\n",
    "                     y_test=np.ravel(y_test)\n",
    "          \n",
    "                     #apothikeush sth lista apotelesmatwn\n",
    "                     apotelesmata_voting_nea [\"key0\"][p+1].extend([y_test])\n",
    "          \n",
    "            \n",
    "            \n",
    "           toc = time.perf_counter()\n",
    "           time_of=toc-tic\n",
    "           print('o xronos poy ekane einai', time_of)\n",
    "           #apothikeush sth lista apotelesmatwn\n",
    "           apotelesmata_voting_nea [\"key0\"][p+2].extend([time_of])\n",
    "           p=p+3;\n",
    "           print('~~teleiwse to K-fold noymero~~',j )\n",
    "           print('xronos=',time_of)\n",
    "           #save the data\n",
    "           with open(r'C:\\Users\\john\\Desktop\\machine learning\\apotelesmata\\duo_dataset\\apotelesmata_voting_nea_' + str(only_names[t]) + '.p' , 'wb') as fp:\n",
    "                    pickle.dump(apotelesmata_voting_nea, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dhmiourgia dic poy tha fortwsoyme ta arxeia apo to fakelo\n",
    "data_clf = {}\n",
    "data_xgboost = {}\n",
    "data_lightgbm = {}\n",
    "data_catboost = {}\n",
    "data_etoima_ola={}\n",
    "\n",
    "#save the names of datasets to only_names\n",
    "only_names_clf = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/apotelesmata/clf/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/apotelesmata/clf/\", f))]\n",
    "only_names_xgboost = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/apotelesmata/xgboost nea/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/apotelesmata/xgboost nea/\", f))]\n",
    "only_names_lightgbm = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/apotelesmata/lightgbm nea/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/apotelesmata/lightgbm nea/\", f))]\n",
    "only_names_catboost = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/apotelesmata/catboost nea/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/apotelesmata/catboost nea/\", f))]\n",
    "only_names_voting_nea = [f for f in listdir(\"C:/Users/John/Desktop/machine learning/apotelesmata/voting_nea/\") if isfile(join(\"C:/Users/John/Desktop/machine learning/apotelesmata/voting_nea/\", f))]\n",
    "\n",
    "#load the data from folder\n",
    "for i in range (26) :\n",
    "    with open(r'C:/Users/John/Desktop/machine learning/apotelesmata/clf/'+str(only_names_clf[i]), 'rb') as fp:\n",
    "     data_clf[str(only_names_clf[i])] = pickle.load(fp)\n",
    "    with open(r'C:/Users/John/Desktop/machine learning/apotelesmata/xgboost nea/'+str(only_names_xgboost[i]), 'rb') as fp:\n",
    "     data_xgboost[str(only_names_xgboost[i])] = pickle.load(fp)\n",
    "    with open(r'C:/Users/John/Desktop/machine learning/apotelesmata/lightgbm nea/'+str(only_names_lightgbm[i]), 'rb') as fp:\n",
    "     data_lightgbm[str(only_names_lightgbm[i])] = pickle.load(fp)\n",
    "    with open(r'C:/Users/John/Desktop/machine learning/apotelesmata/catboost nea/'+str(only_names_catboost[i]), 'rb') as fp:\n",
    "     data_catboost[str(only_names_catboost[i])] = pickle.load(fp)\n",
    "    with open(r'C:/Users/John/Desktop/machine learning/apotelesmata/voting_nea/'+str(only_names_voting_nea[i]), 'rb') as fp:\n",
    "     data_etoima_ola[str(only_names_voting_nea[i])] = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mono gia to voting dokimh\n",
    "etoima_xgboost={}\n",
    "xronos_xgboost={}\n",
    "\n",
    "etoima_lightgbm={}\n",
    "xronos_lightgbm={}\n",
    "\n",
    "etoima_catboost={}\n",
    "xronos_catboost={}\n",
    "\n",
    "etoima_clf_AdaBoost={}\n",
    "xronos_clf_AdaBoost={}\n",
    "\n",
    "etoima_clf_Bagging={}\n",
    "xronos_clf_Bagging={}\n",
    "\n",
    "etoima_clf_ExtraTrees={}\n",
    "xronos_clf_ExtraTrees={}\n",
    "\n",
    "etoima_clf_RandomForest={}\n",
    "xronos_clf_RandomForest={}\n",
    "\n",
    "etoima_clf_Stacking={}\n",
    "xronos_clf_Stacking={}\n",
    "\n",
    "etoima_clf_Voting={}\n",
    "xronos_clf_Voting={}\n",
    "\n",
    "etoima_clf_GradientBoosting={}\n",
    "xronos_clf_GradientBoosting={}\n",
    "\n",
    "etoima_ola={} #11\n",
    "xronos_etoima_ola={} #11\n",
    "\n",
    "\n",
    "for i in range (26) :\n",
    "    x_xgboost=data_xgboost[str(only_names_xgboost[i])]['key0']\n",
    "    \n",
    "    x_lightgbm=data_lightgbm[str(only_names_lightgbm[i])]['key0']\n",
    "    \n",
    "    x_catboost=data_catboost[str(only_names_catboost[i])]['key0']\n",
    "    \n",
    "    x_etoima_ola=data_etoima_ola[str(only_names_voting_nea[i])]['key0'] #11 prosoxh exw valei mono 0\n",
    "    \n",
    "    \n",
    "    x_clf_AdaBoost=data_clf[str(only_names_clf[i])]['key0']\n",
    "    x_clf_Bagging=data_clf[str(only_names_clf[i])]['key1']\n",
    "    x_clf_ExtraTrees=data_clf[str(only_names_clf[i])]['key2']\n",
    "    x_clf_RandomForest=data_clf[str(only_names_clf[i])]['key3']\n",
    "    x_clf_Stacking=data_clf[str(only_names_clf[i])]['key4']\n",
    "    x_clf_Voting=data_clf[str(only_names_clf[i])]['key5']\n",
    "    x_clf_GradientBoosting=data_clf[str(only_names_clf[i])]['key6']\n",
    "    \n",
    "    t=0\n",
    "    \n",
    "    merged_xgboost=[[],[],[],[],[]]\n",
    "    merged1_xgboost=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf=[[],[],[],[],[]]\n",
    "    merged1_clf=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_lightgbm=[[],[],[],[],[]]\n",
    "    merged1_lightgbm=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_catboost=[[],[],[],[],[]]\n",
    "    merged1_catboost=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_AdaBoost=[[],[],[],[],[]]\n",
    "    merged1_clf_AdaBoost=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_Bagging=[[],[],[],[],[]]\n",
    "    merged1_clf_Bagging=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_ExtraTrees=[[],[],[],[],[]]\n",
    "    merged1_clf_ExtraTrees=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_RandomForest=[[],[],[],[],[]]\n",
    "    merged1_clf_RandomForest=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_Stacking=[[],[],[],[],[]]\n",
    "    merged1_clf_Stacking=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_Voting=[[],[],[],[],[]]\n",
    "    merged1_clf_Voting=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_clf_GradientBoosting=[[],[],[],[],[]]\n",
    "    merged1_clf_GradientBoosting=[[],[],[],[],[]]\n",
    "    \n",
    "    merged_etoima_ola=[[],[],[],[],[]]\n",
    "    merged1_etoima_ola=[[],[],[],[],[]]\n",
    "    \n",
    "    for j in range (5) :\n",
    "      merged_xgboost[j]= [item for sublist in x_xgboost[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_xgboost[j]= [item for sublist in x_xgboost[t+1] for item in sublist] #periexei ta true labels\n",
    "        \n",
    "      merged_lightgbm[j]= [item for sublist in x_lightgbm[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_lightgbm[j]= [item for sublist in x_lightgbm[t+1] for item in sublist] #periexei ta true labels\n",
    "        \n",
    "      merged_catboost[j]= [item for sublist in x_catboost[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_catboost[j]= [item for sublist in x_catboost[t+1] for item in sublist] #periexei ta true labels\n",
    "        \n",
    "      merged_clf_AdaBoost[j]= [item for sublist in x_clf_AdaBoost[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_AdaBoost[j]= [item for sublist in x_clf_AdaBoost[t+1] for item in sublist] #periexei ta true labels\n",
    "\n",
    "      merged_clf_Bagging[j]= [item for sublist in x_clf_Bagging[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_Bagging[j]= [item for sublist in x_clf_Bagging[t+1] for item in sublist] #periexei ta true labels\n",
    "        \n",
    "      merged_clf_ExtraTrees[j]= [item for sublist in x_clf_ExtraTrees[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_ExtraTrees[j]= [item for sublist in x_clf_ExtraTrees[t+1] for item in sublist] #periexei ta true labels        \n",
    "\n",
    "      merged_clf_RandomForest[j]= [item for sublist in x_clf_RandomForest[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_RandomForest[j]= [item for sublist in x_clf_RandomForest[t+1] for item in sublist] #periexei ta true labels  \n",
    "        \n",
    "      merged_clf_Stacking[j]= [item for sublist in x_clf_Stacking[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_Stacking[j]= [item for sublist in x_clf_Stacking[t+1] for item in sublist] #periexei ta true labels \n",
    "        \n",
    "      merged_clf_Voting[j]= [item for sublist in x_clf_Voting[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_Voting[j]= [item for sublist in x_clf_Voting[t+1] for item in sublist] #periexei ta true labels\n",
    "        \n",
    "      merged_clf_GradientBoosting[j]= [item for sublist in x_clf_GradientBoosting[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_clf_GradientBoosting[j]= [item for sublist in x_clf_GradientBoosting[t+1] for item in sublist] #periexei ta true labels  \n",
    "      \n",
    "      merged_etoima_ola[j]=[item for sublist in x_etoima_ola[t] for item in sublist] #periexei ta predicted apotelesmata\n",
    "      merged1_etoima_ola[j]=[item for sublist in x_etoima_ola[t+1] for item in sublist] #periexei ta true labels\n",
    "      \n",
    "        \n",
    "      t=t+3\n",
    "    #dhmiourgia dic gia apothikeysh twn merged listwn. prwta einai h lista merged, meta 3 medhinika kai meta h lista merged1\n",
    "    etoima_xgboost[str(only_names_xgboost[i])]= [merged_xgboost,0,0,0,merged1_xgboost]\n",
    "    xronos_xgboost[str(only_names_xgboost[i])]=[x_xgboost[2],x_xgboost[5],x_xgboost[8],x_xgboost[11],x_xgboost[14]]\n",
    "    \n",
    "    etoima_lightgbm[str(only_names_lightgbm[i])]= [merged_lightgbm,0,0,0,merged1_lightgbm]\n",
    "    xronos_lightgbm[str(only_names_lightgbm[i])]=[x_lightgbm[2],x_lightgbm[5],x_lightgbm[8],x_lightgbm[11],x_lightgbm[14]]\n",
    "    \n",
    "    etoima_catboost[str(only_names_catboost[i])]= [merged_catboost,0,0,0,merged1_catboost] \n",
    "    xronos_catboost[str(only_names_catboost[i])]=[x_catboost[2],x_catboost[5],x_catboost[8],x_catboost[11],x_catboost[14]]\n",
    "    \n",
    "    etoima_clf_AdaBoost[str(only_names_clf[i])]= [merged_clf_AdaBoost,0,0,0,merged1_clf_AdaBoost]\n",
    "    xronos_clf_AdaBoost[str(only_names_clf[i])]=[x_clf_AdaBoost[2],x_clf_AdaBoost[5],x_clf_AdaBoost[8],x_clf_AdaBoost[11],x_clf_AdaBoost[14]]\n",
    "    \n",
    "    etoima_clf_Bagging[str(only_names_clf[i])]= [merged_clf_Bagging,0,0,0,merged1_clf_Bagging]\n",
    "    xronos_clf_Bagging[str(only_names_clf[i])]=[x_clf_Bagging[2],x_clf_Bagging[5],x_clf_Bagging[8],x_clf_Bagging[11],x_clf_Bagging[14]]\n",
    "    \n",
    "    etoima_clf_ExtraTrees[str(only_names_clf[i])]= [merged_clf_ExtraTrees,0,0,0,merged1_clf_ExtraTrees]\n",
    "    xronos_clf_ExtraTrees[str(only_names_clf[i])]=[x_clf_ExtraTrees[2],x_clf_ExtraTrees[5],x_clf_ExtraTrees[8],x_clf_ExtraTrees[11],x_clf_ExtraTrees[14]]\n",
    "    \n",
    "    etoima_clf_RandomForest[str(only_names_clf[i])]= [merged_clf_RandomForest,0,0,0,merged1_clf_RandomForest]\n",
    "    xronos_clf_RandomForest[str(only_names_clf[i])]=[x_clf_RandomForest[2],x_clf_RandomForest[5],x_clf_RandomForest[8],x_clf_RandomForest[11],x_clf_RandomForest[14]]\n",
    "    \n",
    "    etoima_clf_Stacking[str(only_names_clf[i])]= [merged_clf_Stacking,0,0,0,merged1_clf_Stacking]\n",
    "    xronos_clf_Stacking[str(only_names_clf[i])]=[x_clf_Stacking[2],x_clf_Stacking[5],x_clf_Stacking[8],x_clf_Stacking[11],x_clf_Stacking[14]]\n",
    "    \n",
    "    etoima_clf_Voting[str(only_names_clf[i])]= [merged_clf_Voting,0,0,0,merged1_clf_Voting]\n",
    "    xronos_clf_Voting[str(only_names_clf[i])]=[x_clf_Voting[2],x_clf_Voting[5],x_clf_Voting[8],x_clf_Voting[11],x_clf_Voting[14]]\n",
    "    \n",
    "    etoima_clf_GradientBoosting[str(only_names_clf[i])]= [merged_clf_GradientBoosting,0,0,0,merged1_clf_GradientBoosting]\n",
    "    xronos_clf_GradientBoosting[str(only_names_clf[i])]=[x_clf_GradientBoosting[2],x_clf_GradientBoosting[5],x_clf_GradientBoosting[8],x_clf_GradientBoosting[11],x_clf_GradientBoosting[14]]\n",
    "    \n",
    "    etoima_ola[str(only_names_voting_nea[i])]=[merged_etoima_ola,0,0,0,merged1_etoima_ola] #prosoxh sto pointer, exw valei 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ypologismos xronou mean-std\n",
    "xronos_mean_std_xgboost={}\n",
    "xronos_mean_std_catboost={}\n",
    "xronos_mean_std_AdaBoost={}\n",
    "xronos_mean_std_Bagging={}\n",
    "xronos_mean_std_ExtraTrees={}\n",
    "xronos_mean_std_RandomForest={}\n",
    "xronos_mean_std_Stacking={}\n",
    "xronos_mean_std_Voting={}\n",
    "xronos_mean_std_GradientBoosting={}\n",
    "\n",
    "\n",
    "for i in range (26) :\n",
    "    xronos_mean_std_xgboost[str(only_names_xgboost[i])]=[np.mean(xronos_xgboost[str(only_names_xgboost[i])]),np.std(xronos_xgboost[str(only_names_xgboost[i])])]\n",
    "    \n",
    "    xronos_mean_std_lightgbm[str(only_names_lightgbm[i])]=[np.mean(xronos_lightgbm[str(only_names_lightgbm[i])]),np.std(xronos_lightgbm[str(only_names_lightgbm[i])])]\n",
    "\n",
    "    xronos_mean_std_catboost[str(only_names_catboost[i])]=[np.mean(xronos_catboost[str(only_names_catboost[i])]),np.std(xronos_catboost[str(only_names_catboost[i])])]\n",
    "    \n",
    "    xronos_mean_std_AdaBoost[str(only_names_clf[i])]=[np.mean(xronos_clf_AdaBoost[str(only_names_clf[i])]),np.std(xronos_clf_AdaBoost[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_Bagging[str(only_names_clf[i])]=[np.mean(xronos_clf_Bagging[str(only_names_clf[i])]),np.std(xronos_clf_Bagging[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_ExtraTrees[str(only_names_clf[i])]=[np.mean(xronos_clf_ExtraTrees[str(only_names_clf[i])]),np.std(xronos_clf_ExtraTrees[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_RandomForest[str(only_names_clf[i])]=[np.mean(xronos_clf_RandomForest[str(only_names_clf[i])]),np.std(xronos_clf_RandomForest[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_Stacking[str(only_names_clf[i])]=[np.mean(xronos_clf_Stacking[str(only_names_clf[i])]),np.std(xronos_clf_Stacking[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_Voting[str(only_names_clf[i])]=[np.mean(xronos_clf_Voting[str(only_names_clf[i])]),np.std(xronos_clf_Voting[str(only_names_clf[i])])]\n",
    "    \n",
    "    xronos_mean_std_GradientBoosting[str(only_names_clf[i])]=[np.mean(xronos_clf_GradientBoosting[str(only_names_clf[i])]),np.std(xronos_clf_GradientBoosting[str(only_names_clf[i])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set the metric  \n",
    "metric=accuracy_score  #  ~f1_score,accuracy_score,specifity --> alla3e kai to fakelo pou apothikeueis\n",
    "\n",
    "apotelesmata_ola={}\n",
    "apotelesmata_XGBoost={}\n",
    "apotelesmata_lightgbm={}\n",
    "apotelesmata_catboost={}\n",
    "apotelesmata_AdaBoost={}\n",
    "apotelesmata_Bagging={}\n",
    "apotelesmata_ExtraTrees={}\n",
    "apotelesmata_RandomForest={}\n",
    "apotelesmata_Stacking={}\n",
    "apotelesmata_Voting={}\n",
    "apotelesmata_GradientBoosting={}\n",
    "\n",
    "apotelesmata_ana_dataset={}\n",
    "\n",
    "for i in range (26):\n",
    "    \n",
    "    a= metric(etoima_xgboost[str(only_names_xgboost[i])][0][0],etoima_xgboost[str(only_names_xgboost[i])][4][0])\n",
    "    a1= metric(etoima_xgboost[str(only_names_xgboost[i])][0][1],etoima_xgboost[str(only_names_xgboost[i])][4][1])\n",
    "    a2= metric(etoima_xgboost[str(only_names_xgboost[i])][0][2],etoima_xgboost[str(only_names_xgboost[i])][4][2])\n",
    "    a3= metric(etoima_xgboost[str(only_names_xgboost[i])][0][3],etoima_xgboost[str(only_names_xgboost[i])][4][3])\n",
    "    a4= metric(etoima_xgboost[str(only_names_xgboost[i])][0][4],etoima_xgboost[str(only_names_xgboost[i])][4][4])\n",
    "\n",
    "    b= metric(etoima_lightgbm[str(only_names_lightgbm[i])][0][0],etoima_lightgbm[str(only_names_lightgbm[i])][4][0])\n",
    "    b1= metric(etoima_lightgbm[str(only_names_lightgbm[i])][0][1],etoima_lightgbm[str(only_names_lightgbm[i])][4][1])\n",
    "    b2= metric(etoima_lightgbm[str(only_names_lightgbm[i])][0][2],etoima_lightgbm[str(only_names_lightgbm[i])][4][2])\n",
    "    b3= metric(etoima_lightgbm[str(only_names_lightgbm[i])][0][3],etoima_lightgbm[str(only_names_lightgbm[i])][4][3])\n",
    "    b4= metric(etoima_lightgbm[str(only_names_lightgbm[i])][0][4],etoima_lightgbm[str(only_names_lightgbm[i])][4][4])\n",
    "\n",
    "    c= metric(etoima_catboost[str(only_names_catboost[i])][0][0],etoima_catboost[str(only_names_catboost[i])][4][0])\n",
    "    c1= metric(etoima_catboost[str(only_names_catboost[i])][0][1],etoima_catboost[str(only_names_catboost[i])][4][1])\n",
    "    c2= metric(etoima_catboost[str(only_names_catboost[i])][0][2],etoima_catboost[str(only_names_catboost[i])][4][2])\n",
    "    c3= metric(etoima_catboost[str(only_names_catboost[i])][0][3],etoima_catboost[str(only_names_catboost[i])][4][3])\n",
    "    c4= metric(etoima_catboost[str(only_names_catboost[i])][0][4],etoima_catboost[str(only_names_catboost[i])][4][4])\n",
    "\n",
    "    d= metric(etoima_clf_AdaBoost[str(only_names_clf[i])][0][0],etoima_clf_AdaBoost[str(only_names_clf[i])][4][0])\n",
    "    d1= metric(etoima_clf_AdaBoost[str(only_names_clf[i])][0][1],etoima_clf_AdaBoost[str(only_names_clf[i])][4][1])\n",
    "    d2= metric(etoima_clf_AdaBoost[str(only_names_clf[i])][0][2],etoima_clf_AdaBoost[str(only_names_clf[i])][4][2])\n",
    "    d3= metric(etoima_clf_AdaBoost[str(only_names_clf[i])][0][3],etoima_clf_AdaBoost[str(only_names_clf[i])][4][3])\n",
    "    d4= metric(etoima_clf_AdaBoost[str(only_names_clf[i])][0][4],etoima_clf_AdaBoost[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    e= metric(etoima_clf_Bagging[str(only_names_clf[i])][0][0],etoima_clf_Bagging[str(only_names_clf[i])][4][0])\n",
    "    e1= metric(etoima_clf_Bagging[str(only_names_clf[i])][0][1],etoima_clf_Bagging[str(only_names_clf[i])][4][1])\n",
    "    e2= metric(etoima_clf_Bagging[str(only_names_clf[i])][0][2],etoima_clf_Bagging[str(only_names_clf[i])][4][2])\n",
    "    e3= metric(etoima_clf_Bagging[str(only_names_clf[i])][0][3],etoima_clf_Bagging[str(only_names_clf[i])][4][3])\n",
    "    e4= metric(etoima_clf_Bagging[str(only_names_clf[i])][0][4],etoima_clf_Bagging[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    f= metric(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][0],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][0])\n",
    "    f1= metric(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][1],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][1])\n",
    "    f2= metric(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][2],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][2])\n",
    "    f3= metric(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][3],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][3])\n",
    "    f4= metric(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][4],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    g= metric(etoima_clf_RandomForest[str(only_names_clf[i])][0][0],etoima_clf_RandomForest[str(only_names_clf[i])][4][0])\n",
    "    g1= metric(etoima_clf_RandomForest[str(only_names_clf[i])][0][1],etoima_clf_RandomForest[str(only_names_clf[i])][4][1])\n",
    "    g2= metric(etoima_clf_RandomForest[str(only_names_clf[i])][0][2],etoima_clf_RandomForest[str(only_names_clf[i])][4][2])\n",
    "    g3= metric(etoima_clf_RandomForest[str(only_names_clf[i])][0][3],etoima_clf_RandomForest[str(only_names_clf[i])][4][3])\n",
    "    g4= metric(etoima_clf_RandomForest[str(only_names_clf[i])][0][4],etoima_clf_RandomForest[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    h= metric(etoima_clf_Stacking[str(only_names_clf[i])][0][0],etoima_clf_Stacking[str(only_names_clf[i])][4][0])\n",
    "    h1= metric(etoima_clf_Stacking[str(only_names_clf[i])][0][1],etoima_clf_Stacking[str(only_names_clf[i])][4][1])\n",
    "    h2= metric(etoima_clf_Stacking[str(only_names_clf[i])][0][2],etoima_clf_Stacking[str(only_names_clf[i])][4][2])\n",
    "    h3= metric(etoima_clf_Stacking[str(only_names_clf[i])][0][3],etoima_clf_Stacking[str(only_names_clf[i])][4][3])\n",
    "    h4= metric(etoima_clf_Stacking[str(only_names_clf[i])][0][4],etoima_clf_Stacking[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    m= metric(etoima_clf_Voting[str(only_names_clf[i])][0][0],etoima_clf_Voting[str(only_names_clf[i])][4][0])\n",
    "    m1= metric(etoima_clf_Voting[str(only_names_clf[i])][0][1],etoima_clf_Voting[str(only_names_clf[i])][4][1])\n",
    "    m2= metric(etoima_clf_Voting[str(only_names_clf[i])][0][2],etoima_clf_Voting[str(only_names_clf[i])][4][2])\n",
    "    m3= metric(etoima_clf_Voting[str(only_names_clf[i])][0][3],etoima_clf_Voting[str(only_names_clf[i])][4][3])\n",
    "    m4= metric(etoima_clf_Voting[str(only_names_clf[i])][0][4],etoima_clf_Voting[str(only_names_clf[i])][4][4])\n",
    "\n",
    "    j= metric(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][0],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][0])\n",
    "    j1= metric(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][1],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][1])\n",
    "    j2= metric(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][2],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][2])\n",
    "    j3= metric(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][3],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][3])\n",
    "    j4= metric(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][4],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][4])\n",
    "\n",
    "  #exw peira3ei to pointer, na douleuei mono gia prwto arxeio  \n",
    "    l= metric(etoima_ola[str(only_names_voting_nea[i])][0][0],etoima_ola[str(only_names_voting_nea[i])][4][0])\n",
    "    l1= metric(etoima_ola[str(only_names_voting_nea[i])][0][1],etoima_ola[str(only_names_voting_nea[i])][4][1])\n",
    "    l2= metric(etoima_ola[str(only_names_voting_nea[i])][0][2],etoima_ola[str(only_names_voting_nea[i])][4][2])\n",
    "    l3= metric(etoima_ola[str(only_names_voting_nea[i])][0][3],etoima_ola[str(only_names_voting_nea[i])][4][3])\n",
    "    l4= metric(etoima_ola[str(only_names_voting_nea[i])][0][4],etoima_ola[str(only_names_voting_nea[i])][4][4])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    apotelesmata_XGBoost[str(only_names_xgboost[i])]=[a,a1,a2,a3,a4]\n",
    "    \n",
    "    apotelesmata_lightgbm[str(only_names_lightgbm[i])]=[b,b1,b2,b3,b4]\n",
    "    \n",
    "    apotelesmata_catboost[str(only_names_catboost[i])]=[c,c1,c2,c3,c4]\n",
    "    \n",
    "    apotelesmata_AdaBoost[str(only_names_clf[i])]=[d,d1,d2,d3,d4]\n",
    "    \n",
    "    apotelesmata_Bagging[str(only_names_clf[i])]=[e,e1,e2,e3,e4]\n",
    "    \n",
    "    apotelesmata_ExtraTrees[str(only_names_clf[i])]=[f,f1,f2,f3,f4]\n",
    "    \n",
    "    apotelesmata_RandomForest[str(only_names_clf[i])]=[g,g1,g2,g3,g4]\n",
    "    \n",
    "    apotelesmata_Stacking[str(only_names_clf[i])]=[h,h1,h2,h3,h4]\n",
    "    \n",
    "    apotelesmata_Voting[str(only_names_clf[i])]=[m,m1,m2,m3,m4]\n",
    "    \n",
    "    apotelesmata_GradientBoosting[str(only_names_clf[i])]=[j,j1,j2,j3,j4]\n",
    "    \n",
    "    apotelesmata_ola[str(only_names_voting_nea[i])]=[l,l1,l2,l3,l4] \n",
    "    \n",
    "    apotelesmata_ana_dataset[str(only_names_clf[i])]= [apotelesmata_ola[str(only_names_voting_nea[i])],apotelesmata_XGBoost[str(only_names_xgboost[i])],apotelesmata_lightgbm[str(only_names_lightgbm[i])],apotelesmata_catboost[str(only_names_catboost[i])],apotelesmata_AdaBoost[str(only_names_clf[i])],apotelesmata_Bagging[str(only_names_clf[i])],apotelesmata_ExtraTrees[str(only_names_clf[i])],apotelesmata_RandomForest[str(only_names_clf[i])],apotelesmata_Stacking[str(only_names_clf[i])],apotelesmata_Voting[str(only_names_clf[i])],apotelesmata_GradientBoosting[str(only_names_clf[i])]]\n",
    "\n",
    "    \n",
    "    \n",
    "apotelesmata_df=pd.DataFrame.from_dict(apotelesmata_ana_dataset)\n",
    "\n",
    "apotelesmata_df.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/dokimh_excel/accuracy_score_work.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ypologismos specificity\n",
    "\n",
    "\n",
    "apotelesmata_XGBoost={}\n",
    "apotelesmata_lightgbm={}\n",
    "apotelesmata_catboost={}\n",
    "apotelesmata_AdaBoost={}\n",
    "apotelesmata_Bagging={}\n",
    "apotelesmata_ExtraTrees={}\n",
    "apotelesmata_RandomForest={}\n",
    "apotelesmata_Stacking={}\n",
    "apotelesmata_Voting={}\n",
    "apotelesmata_GradientBoosting={}\n",
    "\n",
    "apotelesmata_ana_dataset={}\n",
    "\n",
    "for i in range (26):\n",
    "    a= ypologismos_specificity(multilabel_confusion_matrix(etoima_xgboost[str(only_names_xgboost[i])][0][0],etoima_xgboost[str(only_names_xgboost[i])][4][0]).ravel())\n",
    "    a1= ypologismos_specificity(multilabel_confusion_matrix(etoima_xgboost[str(only_names_xgboost[i])][0][1],etoima_xgboost[str(only_names_xgboost[i])][4][1]).ravel())\n",
    "    a2= ypologismos_specificity(multilabel_confusion_matrix(etoima_xgboost[str(only_names_xgboost[i])][0][2],etoima_xgboost[str(only_names_xgboost[i])][4][2]).ravel())\n",
    "    a3= ypologismos_specificity(multilabel_confusion_matrix(etoima_xgboost[str(only_names_xgboost[i])][0][3],etoima_xgboost[str(only_names_xgboost[i])][4][3]).ravel())\n",
    "    a4= ypologismos_specificity(multilabel_confusion_matrix(etoima_xgboost[str(only_names_xgboost[i])][0][4],etoima_xgboost[str(only_names_xgboost[i])][4][4]).ravel())\n",
    "\n",
    "    b= ypologismos_specificity(multilabel_confusion_matrix(etoima_lightgbm[str(only_names_lightgbm[i])][0][0],etoima_lightgbm[str(only_names_lightgbm[i])][4][0]).ravel())\n",
    "    b1= ypologismos_specificity(multilabel_confusion_matrix(etoima_lightgbm[str(only_names_lightgbm[i])][0][1],etoima_lightgbm[str(only_names_lightgbm[i])][4][1]).ravel())\n",
    "    b2= ypologismos_specificity(multilabel_confusion_matrix(etoima_lightgbm[str(only_names_lightgbm[i])][0][2],etoima_lightgbm[str(only_names_lightgbm[i])][4][2]).ravel())\n",
    "    b3= ypologismos_specificity(multilabel_confusion_matrix(etoima_lightgbm[str(only_names_lightgbm[i])][0][3],etoima_lightgbm[str(only_names_lightgbm[i])][4][3]).ravel())\n",
    "    b4= ypologismos_specificity(multilabel_confusion_matrix(etoima_lightgbm[str(only_names_lightgbm[i])][0][4],etoima_lightgbm[str(only_names_lightgbm[i])][4][4]).ravel())\n",
    "\n",
    "    c= ypologismos_specificity(multilabel_confusion_matrix(etoima_catboost[str(only_names_catboost[i])][0][0],etoima_catboost[str(only_names_catboost[i])][4][0]).ravel())\n",
    "    c1= ypologismos_specificity(multilabel_confusion_matrix(etoima_catboost[str(only_names_catboost[i])][0][1],etoima_catboost[str(only_names_catboost[i])][4][1]).ravel())\n",
    "    c2= ypologismos_specificity(multilabel_confusion_matrix(etoima_catboost[str(only_names_catboost[i])][0][2],etoima_catboost[str(only_names_catboost[i])][4][2]).ravel())\n",
    "    c3= ypologismos_specificity(multilabel_confusion_matrix(etoima_catboost[str(only_names_catboost[i])][0][3],etoima_catboost[str(only_names_catboost[i])][4][3]).ravel())\n",
    "    c4= ypologismos_specificity(multilabel_confusion_matrix(etoima_catboost[str(only_names_catboost[i])][0][4],etoima_catboost[str(only_names_catboost[i])][4][4]).ravel())\n",
    "\n",
    "    d= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_AdaBoost[str(only_names_clf[i])][0][0],etoima_clf_AdaBoost[str(only_names_clf[i])][4][0]).ravel())\n",
    "    d1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_AdaBoost[str(only_names_clf[i])][0][1],etoima_clf_AdaBoost[str(only_names_clf[i])][4][1]).ravel())\n",
    "    d2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_AdaBoost[str(only_names_clf[i])][0][2],etoima_clf_AdaBoost[str(only_names_clf[i])][4][2]).ravel())\n",
    "    d3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_AdaBoost[str(only_names_clf[i])][0][3],etoima_clf_AdaBoost[str(only_names_clf[i])][4][3]).ravel())\n",
    "    d4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_AdaBoost[str(only_names_clf[i])][0][4],etoima_clf_AdaBoost[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    e= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Bagging[str(only_names_clf[i])][0][0],etoima_clf_Bagging[str(only_names_clf[i])][4][0]).ravel())\n",
    "    e1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Bagging[str(only_names_clf[i])][0][1],etoima_clf_Bagging[str(only_names_clf[i])][4][1]).ravel())\n",
    "    e2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Bagging[str(only_names_clf[i])][0][2],etoima_clf_Bagging[str(only_names_clf[i])][4][2]).ravel())\n",
    "    e3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Bagging[str(only_names_clf[i])][0][3],etoima_clf_Bagging[str(only_names_clf[i])][4][3]).ravel())\n",
    "    e4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Bagging[str(only_names_clf[i])][0][4],etoima_clf_Bagging[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    f= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][0],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][0]).ravel())\n",
    "    f1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][1],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][1]).ravel())\n",
    "    f2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][2],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][2]).ravel())\n",
    "    f3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][3],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][3]).ravel())\n",
    "    f4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_ExtraTrees[str(only_names_clf[i])][0][4],etoima_clf_ExtraTrees[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    g= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_RandomForest[str(only_names_clf[i])][0][0],etoima_clf_RandomForest[str(only_names_clf[i])][4][0]).ravel())\n",
    "    g1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_RandomForest[str(only_names_clf[i])][0][1],etoima_clf_RandomForest[str(only_names_clf[i])][4][1]).ravel())\n",
    "    g2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_RandomForest[str(only_names_clf[i])][0][2],etoima_clf_RandomForest[str(only_names_clf[i])][4][2]).ravel())\n",
    "    g3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_RandomForest[str(only_names_clf[i])][0][3],etoima_clf_RandomForest[str(only_names_clf[i])][4][3]).ravel())\n",
    "    g4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_RandomForest[str(only_names_clf[i])][0][4],etoima_clf_RandomForest[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    h= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Stacking[str(only_names_clf[i])][0][0],etoima_clf_Stacking[str(only_names_clf[i])][4][0]).ravel())\n",
    "    h1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Stacking[str(only_names_clf[i])][0][1],etoima_clf_Stacking[str(only_names_clf[i])][4][1]).ravel())\n",
    "    h2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Stacking[str(only_names_clf[i])][0][2],etoima_clf_Stacking[str(only_names_clf[i])][4][2]).ravel())\n",
    "    h3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Stacking[str(only_names_clf[i])][0][3],etoima_clf_Stacking[str(only_names_clf[i])][4][3]).ravel())\n",
    "    h4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Stacking[str(only_names_clf[i])][0][4],etoima_clf_Stacking[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    m= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Voting[str(only_names_clf[i])][0][0],etoima_clf_Voting[str(only_names_clf[i])][4][0]).ravel())\n",
    "    m1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Voting[str(only_names_clf[i])][0][1],etoima_clf_Voting[str(only_names_clf[i])][4][1]).ravel())\n",
    "    m2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Voting[str(only_names_clf[i])][0][2],etoima_clf_Voting[str(only_names_clf[i])][4][2]).ravel())\n",
    "    m3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Voting[str(only_names_clf[i])][0][3],etoima_clf_Voting[str(only_names_clf[i])][4][3]).ravel())\n",
    "    m4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_Voting[str(only_names_clf[i])][0][4],etoima_clf_Voting[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    j= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][0],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][0]).ravel())\n",
    "    j1= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][1],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][1]).ravel())\n",
    "    j2= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][2],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][2]).ravel())\n",
    "    j3= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][3],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][3]).ravel())\n",
    "    j4= ypologismos_specificity(multilabel_confusion_matrix(etoima_clf_GradientBoosting[str(only_names_clf[i])][0][4],etoima_clf_GradientBoosting[str(only_names_clf[i])][4][4]).ravel())\n",
    "\n",
    "    \n",
    "\n",
    "    l= ypologismos_specificity(multilabel_confusion_matrix(etoima_ola[str(only_names_voting_nea[i])][0][0],etoima_ola[str(only_names_voting_nea[i])][4][0]).ravel())\n",
    "    l1= ypologismos_specificity(multilabel_confusion_matrix(etoima_ola[str(only_names_voting_nea[i])][0][1],etoima_ola[str(only_names_voting_nea[i])][4][1]).ravel())\n",
    "    l2= ypologismos_specificity(multilabel_confusion_matrix(etoima_ola[str(only_names_voting_nea[i])][0][2],etoima_ola[str(only_names_voting_nea[i])][4][2]).ravel())\n",
    "    l3= ypologismos_specificity(multilabel_confusion_matrix(etoima_ola[str(only_names_voting_nea[i])][0][3],etoima_ola[str(only_names_voting_nea[i])][4][3]).ravel())\n",
    "    l4= ypologismos_specificity(multilabel_confusion_matrix(etoima_ola[str(only_names_voting_nea[i])][0][4],etoima_ola[str(only_names_voting_nea[i])][4][4]).ravel())\n",
    "    \n",
    "    \n",
    "    apotelesmata_XGBoost[str(only_names_xgboost[i])]=[a,a1,a2,a3,a4]\n",
    "    \n",
    "    apotelesmata_lightgbm[str(only_names_lightgbm[i])]=[b,b1,b2,b3,b4]\n",
    "    \n",
    "    apotelesmata_catboost[str(only_names_catboost[i])]=[c,c1,c2,c3,c4]\n",
    "    \n",
    "    apotelesmata_AdaBoost[str(only_names_clf[i])]=[d,d1,d2,d3,d4]\n",
    "    \n",
    "    apotelesmata_Bagging[str(only_names_clf[i])]=[e,e1,e2,e3,e4]\n",
    "    \n",
    "    apotelesmata_ExtraTrees[str(only_names_clf[i])]=[f,f1,f2,f3,f4]\n",
    "    \n",
    "    apotelesmata_RandomForest[str(only_names_clf[i])]=[g,g1,g2,g3,g4]\n",
    "    \n",
    "    apotelesmata_Stacking[str(only_names_clf[i])]=[h,h1,h2,h3,h4]\n",
    "    \n",
    "    apotelesmata_Voting[str(only_names_clf[i])]=[m,m1,m2,m3,m4]\n",
    "    \n",
    "    apotelesmata_GradientBoosting[str(only_names_clf[i])]=[j,j1,j2,j3,j4]\n",
    "    \n",
    "    apotelesmata_ola[str(only_names_voting_nea[i])]=[l,l1,l2,l3,l4]\n",
    "    \n",
    "    apotelesmata_ana_dataset[str(only_names_clf[i])]= [apotelesmata_ola[str(only_names_voting_nea[i])],apotelesmata_XGBoost[str(only_names_xgboost[i])],apotelesmata_lightgbm[str(only_names_lightgbm[i])],apotelesmata_catboost[str(only_names_catboost[i])],apotelesmata_AdaBoost[str(only_names_clf[i])],apotelesmata_Bagging[str(only_names_clf[i])],apotelesmata_ExtraTrees[str(only_names_clf[i])],apotelesmata_RandomForest[str(only_names_clf[i])],apotelesmata_Stacking[str(only_names_clf[i])],apotelesmata_Voting[str(only_names_clf[i])],apotelesmata_GradientBoosting[str(only_names_clf[i])]]\n",
    "\n",
    "apotelesmata_df=pd.DataFrame.from_dict(apotelesmata_ana_dataset)\n",
    "\n",
    "apotelesmata_df.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/specificity_work.xls')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epeidh otan gyrizw ta apotelesmata apo to excel einai ola apothikeumena se string eftia3a funtion na ta gyrizei se float\n",
    "\n",
    "apotelesmata_df_accuracy_score=pd.read_excel('C:/Users/John/Desktop/pros telos apo 14.06/apotelesmata/accuracy_score.xls')\n",
    "\n",
    "apotelesmata_df_f1_score=pd.read_excel('C:/Users/John/Desktop/pros telos apo 14.06/apotelesmata/f1_score.xls') \n",
    "\n",
    "apotelesmata_df_specificity=pd.read_excel('C:/Users/John/Desktop/pros telos apo 14.06/apotelesmata/specificity.xls') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allagh onomatos sto only_names\n",
    "only_names_clf_new=[0]*26\n",
    "for i in range(26):\n",
    "    only_names_clf_new[i]=only_names_clf[i][13:]\n",
    "    only_names_clf_new[i]=only_names_clf_new[i][:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dhmiourgia boxplot\n",
    "for i in range (26):\n",
    " d_1 = {'G3': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][7]), 'Stacking': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][10])}\n",
    " d_2= {'G3': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][6]), 'Stacking': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][10])}\n",
    " d_3={'G3': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][7]), 'Stacking': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][10])}\n",
    "\n",
    " df_1 = pd.DataFrame(d_1)\n",
    " df_2 = pd.DataFrame(d_2)\n",
    " df_3 = pd.DataFrame(d_3)\n",
    "\n",
    " fig, ax = plt.subplots(1,3,figsize=(22.20,7.80)) # define the axis object here\n",
    " fig.suptitle(str(only_names_clf_new[i]))\n",
    " sns.set_style(\"whitegrid\")\n",
    "\n",
    " sns.boxplot(ax=ax[0],data=df_1,showfliers=False,palette=\"Set1\").set_title('accuracy')\n",
    " sns.boxplot(ax=ax[1],data=df_2,showfliers=False,palette=\"Set1\").set_title('f1') \n",
    " sns.boxplot(ax=ax[2],data=df_3,showfliers=False,palette=\"Set1\").set_title('specificity') \n",
    "\n",
    "\n",
    " plt.setp(ax[0].get_xticklabels(), rotation=25) #rotation of labels_x\n",
    " ax[0].yaxis.grid(False) # Hide the horizontal gridlines\n",
    " ax[0].xaxis.grid(True) # Show the vertical gridlines\n",
    "\n",
    " plt.setp(ax[1].get_xticklabels(), rotation=25) #rotation of labels_x\n",
    " ax[1].yaxis.grid(False) # Hide the horizontal gridlines\n",
    " ax[1].xaxis.grid(True) # Show the vertical gridlines\n",
    " \n",
    " plt.setp(ax[2].get_xticklabels(), rotation=25) #rotation of labels_x\n",
    " ax[2].yaxis.grid(False) # Hide the horizontal gridlines\n",
    " ax[2].xaxis.grid(True) # Show the vertical gridlines\n",
    "    \n",
    " plt.savefig('C:/Users/John/Desktop/pros telos apo 14.06/boxplots/'+str(only_names_clf_new[i])+'.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create comparison between algorithms\n",
    "sigkrish={}\n",
    "sigkrish['G3']=[0]*11\n",
    "sigkrish['xgboost']=[0]*11\n",
    "sigkrish['lightgbm']=[0]*11\n",
    "sigkrish['catboost']=[0]*11\n",
    "sigkrish['adaboost']=[0]*11\n",
    "sigkrish['bagging']=[0]*11\n",
    "sigkrish['extratrees']=[0]*11\n",
    "sigkrish['randomforest']=[0]*11\n",
    "sigkrish['stacking']=[0]*11\n",
    "sigkrish['voting']=[0]*11\n",
    "sigkrish['gradientboosting']=[0]*11\n",
    "\n",
    "\n",
    "\n",
    "for i in range (26):\n",
    "      d_1 = {'Voting_G3': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][7]), 'Stacking': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_accuracy_score[str(only_names_clf[i])][10])}\n",
    "      d_2= {'Voting_G3': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][6]), 'Stacking': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_f1_score[str(only_names_clf[i])][10])}\n",
    "      d_3={'Voting_G3': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][0]),'XGBoost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][1]), 'LightGBM': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][2]),'Catboost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][3]), 'AdaBoost': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][4]), 'Bagging': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][5]), 'ExtraTrees': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][6]), 'RandomForest': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][7]), 'Stacking': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][8]), 'Voting': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][9]), 'GradientBoosting': string_to_float(apotelesmata_df_specificity[str(only_names_clf[i])][10])}\n",
    "\n",
    "      df_1 = pd.DataFrame(d_1)\n",
    "      df_2 = pd.DataFrame(d_2)\n",
    "      df_3 = pd.DataFrame(d_3)\n",
    "      \n",
    "      #pick df and set the max values of each column\n",
    "      y=df_3.max()\n",
    "      \n",
    "      #to vhma gia na allazei h sigkrish\n",
    "      p=0\n",
    "      for x,t in sigkrish.items():\n",
    "            \n",
    "            if  y[p]>y[0] :\n",
    "                sigkrish[x][0]=sigkrish[x][0]+1            \n",
    "            if  y[p]>y[1] :\n",
    "                sigkrish[x][1]=sigkrish[x][1]+1\n",
    "            if y[p]>y[2]:\n",
    "                sigkrish[x][2]=sigkrish[x][2]+1\n",
    "            if y[p]>y[3]:\n",
    "                sigkrish[x][3]=sigkrish[x][3]+1\n",
    "            if y[p]>y[4]:\n",
    "                sigkrish[x][4]=sigkrish[x][4]+1\n",
    "            if y[p]>y[5]:\n",
    "                sigkrish[x][5]=sigkrish[x][5]+1\n",
    "            if y[p]>y[6]:\n",
    "                sigkrish[x][6]=sigkrish[x][6]+1\n",
    "            if y[p]>y[7]:\n",
    "                sigkrish[x][7]=sigkrish[x][7]+1\n",
    "            if y[p]>y[8]:\n",
    "                sigkrish[x][8]=sigkrish[x][8]+1\n",
    "            if y[p]>y[9]:\n",
    "                sigkrish[x][9]=sigkrish[x][9]+1\n",
    "            if y[p]>y[10]:\n",
    "                sigkrish[x][10]=sigkrish[x][10]+1\n",
    "                \n",
    "            \n",
    "            p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "sigkrish_1=sigkrish\n",
    "sigkrish_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score\n",
    "sigkrish_2=sigkrish\n",
    "sigkrish_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificity\n",
    "sigkrish_3=sigkrish\n",
    "sigkrish_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sigkrish_1=pd.DataFrame.from_dict(sigkrish_1)\n",
    "df_sigkrish_1 = df_sigkrish_1.T\n",
    "\n",
    "df_sigkrish_2=pd.DataFrame.from_dict(sigkrish_2)\n",
    "df_sigkrish_2 = df_sigkrish_2.T\n",
    "\n",
    "df_sigkrish_3=pd.DataFrame.from_dict(sigkrish_3)\n",
    "df_sigkrish_3 = df_sigkrish_3.T\n",
    "\n",
    "\n",
    "df_sigkrish_1.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/df_sigkrish_1.xls')  \n",
    "df_sigkrish_2.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/df_sigkrish_2.xls') \n",
    "df_sigkrish_3.to_excel('C:/Users/john/Desktop/machine learning/apotelesmata/df_sigkrish_3.xls') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
